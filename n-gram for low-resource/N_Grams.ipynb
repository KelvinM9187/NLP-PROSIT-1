{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Environment and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atq8Gxle9qQg"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oODyJRid_Peg",
        "outputId": "f7aa26ac-5910-4d8d-c27b-350cf55118ae"
      },
      "outputs": [],
      "source": [
        "twi_file = \"twi.txt\"\n",
        "\n",
        "with open(twi_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    twi_sentences = f.readlines()\n",
        "\n",
        "print(\"Number of raw Twi sentences:\", len(twi_sentences))\n",
        "print(\"Sample sentence:\", twi_sentences[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkLpXuh__Yd0"
      },
      "source": [
        "### Clean the Tetxt (Twi Specific)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JGfQhHk_Ruy",
        "outputId": "be6dda2f-62ad-4291-eae4-7e84d8f037b3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def clean_text(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"[^a-zɔɛ\\-\\' ]+\", \"\", sentence)\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
        "    return sentence\n",
        "\n",
        "cleaned_sentences = [clean_text(s) for s in twi_sentences]\n",
        "cleaned_sentences = [s for s in cleaned_sentences if s != \"\"]\n",
        "\n",
        "print(\"Cleaned sentences:\", len(cleaned_sentences))\n",
        "print(\"Example:\", cleaned_sentences[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj_yg0QBPvOw",
        "outputId": "db641527-2e01-47ff-ee80-776fe549d572"
      },
      "outputs": [],
      "source": [
        "with open(\"twi_clean_for_sketch_engine.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in cleaned_sentences:\n",
        "        f.write(sentence + \"\\n\")\n",
        "\n",
        "print(\"File saved: twi_clean_for_sketch_engine.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls465iPILB7X"
      },
      "source": [
        "### Split the Data (80/10/10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71X9K36GbSdq",
        "outputId": "0a478d28-1fd5-4420-b40f-0ef1903f2adc"
      },
      "outputs": [],
      "source": [
        "cleaned_file = \"twi_clean_for_sketch_engine.txt\"\n",
        "\n",
        "with open(cleaned_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    # We read lines and strip to ensure no trailing newlines in our list\n",
        "    full_cleaned_data = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "\n",
        "# --- The Split (80/10/10) ---\n",
        "# First split: 80% train, 20% temp\n",
        "train_sentences, temp_sentences = train_test_split(full_cleaned_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: split temp into 50/50 to get 10% dev and 10% test\n",
        "dev_sentences, test_sentences = train_test_split(temp_sentences, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Split sizes -> Train: {len(train_sentences)}, Dev: {len(dev_sentences)}, Test: {len(test_sentences)}\")\n",
        "\n",
        "\n",
        "# Save train set only for the tokenizer\n",
        "with open(\"twi_train_only.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for s in train_sentences:\n",
        "        f.write(s + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. CUSTOM TWI SUBWORD TOKENIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJTFhJl7LpnU"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "\n",
        "# We use a custom BPE that uses Twi-aware splitting\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "TWI_REGEX = r\"[A-Za-zÀ-ÖØ-öø-ÿƐɛƆɔ0-9]+(?:['-][A-Za-zÀ-ÖØ-öø-ÿƐɛƆɔ0-9]+)*|[^\\w\\s]\"\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
        "    pre_tokenizers.Whitespace(),  # splits & removes spaces\n",
        "    pre_tokenizers.Split(\n",
        "        pattern=TWI_REGEX,\n",
        "        behavior=\"isolated\"\n",
        "    )\n",
        "])\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=3000, # Optimized for low-resource density\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"<s>\", \"</s>\", \"[MASK]\"]\n",
        ")\n",
        "tokenizer.train(files=[\"twi_train_only.txt\"], trainer=trainer)\n",
        "\n",
        "def tokenize_set(data):\n",
        "    return [[\"<s>\"] + tokenizer.encode(s).tokens + [\"</s>\"] for s in data]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a random subset of 10,000 sentences from the test set\n",
        "# This drastically reduces the time for perplexity evaluation\n",
        "sample_size = 1000\n",
        "if len(test_sentences) >= sample_size:\n",
        "    test_sample = random.sample(test_sentences, sample_size)\n",
        "else:\n",
        "    test_sample = test_sentences  # Fallback if the test set is smaller than 10k\n",
        "\n",
        "print(f\"Created a random test sample of {len(test_sample)} sentences.\")\n",
        "\n",
        "# Now tokenize this sample instead of the full test set\n",
        "test_tokenized = tokenize_set(test_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ8xaQQuctDy"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_tokenized = tokenize_set(train_sentences)\n",
        "dev_tokenized = tokenize_set(dev_sentences)\n",
        "test_tokenized = tokenize_set(test_sentences)\n",
        "\n",
        "\n",
        "print(train_tokenized[:30])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IbNCy02Ka8N"
      },
      "source": [
        "## Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "Vz7IUTo1KZua",
        "outputId": "54a43258-99e3-4598-a9bc-db060ca5b0c9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def run_analysis(data):\n",
        "    \"\"\"Linguistic analysis based on Zipf's Law.\"\"\"\n",
        "    all_words = \" \".join(data).split()\n",
        "    counts = Counter(all_words)\n",
        "    freqs = sorted(counts.values(), reverse=True)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Zipf's Law Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.loglog(freqs)\n",
        "    plt.title(\"Zipf's Law (Twi Corpus)\")\n",
        "\n",
        "    # Cumulative Coverage\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot([sum(freqs[:i])/sum(freqs) for i in range(len(freqs[:1000]))])\n",
        "    plt.title(\"Vocab Coverage (Top 1000)\")\n",
        "    plt.show()\n",
        "\n",
        "run_analysis(train_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Optimized Training & Performance Benchmarking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train with WittenBell Interpolated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.lm import WittenBellInterpolated\n",
        "\n",
        "def train_and_save_ngram(train_tokenized, n, model_path):\n",
        "    \"\"\"\n",
        "    Train an n-gram  LM and save it to disk.\n",
        "    \"\"\"\n",
        "    print(f\"Training {n}-gram model...\")\n",
        "\n",
        "    train_data, vocab = padded_everygram_pipeline(n, train_tokenized)\n",
        "\n",
        "    model = WittenBellInterpolated(n)\n",
        "    \n",
        "    model.fit(train_data, vocab)\n",
        "\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    print(f\"Saved model → {model_path}\")\n",
        "\n",
        "\n",
        "n_orders = [3, 4, 5]\n",
        "\n",
        "print(f\"Evaluating {len(test_tokenized)} test sentences\")\n",
        "\n",
        "for n in n_orders:\n",
        "    start_time = time.time()\n",
        "\n",
        "    \n",
        "    model_path = f\"twi_wittenBell_{n}gram.pkl\"\n",
        "\n",
        "    # --- TRAIN & SAVE ---\n",
        "    train_and_save_ngram(train_tokenized, n, model_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Or, Train with Kneser-ney"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.lm import KneserNeyInterpolated\n",
        "\n",
        "def train_and_save_ngram(train_tokenized, n, model_path):\n",
        "    \"\"\"\n",
        "    Train an n-gram Kneser–Ney LM and save it to disk.\n",
        "    \"\"\"\n",
        "    print(f\"Training {n}-gram model...\")\n",
        "\n",
        "    train_data, vocab = padded_everygram_pipeline(n, train_tokenized)\n",
        "\n",
        "    model = KneserNeyInterpolated(n)\n",
        "    \n",
        "    model.fit(train_data, vocab)\n",
        "\n",
        "    with open(model_path, \"wb\") as f:\n",
        "        pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    print(f\"Saved model → {model_path}\")\n",
        "\n",
        "\n",
        "n_orders = [3, 4, 5]\n",
        "\n",
        "print(f\"Evaluating {len(test_tokenized)} test sentences\")\n",
        "\n",
        "for n in n_orders:\n",
        "    start_time = time.time()\n",
        "\n",
        "    \n",
        "    model_path = f\"twi_kneser_ney_{n}gram.pkl\"\n",
        "\n",
        "    # --- TRAIN & SAVE ---\n",
        "    train_and_save_ngram(train_tokenized, n, model_path)\n",
        "\n",
        "n_orders = [3, 4, 5]\n",
        "\n",
        "print(f\"Evaluating {len(test_tokenized)} test sentences\")\n",
        "\n",
        "for n in n_orders:\n",
        "    start_time = time.time()\n",
        "\n",
        "    \n",
        "    model_path = f\"twi_kneser_ney_{n}gram.pkl\"\n",
        "\n",
        "    # --- TRAIN & SAVE ---\n",
        "    train_and_save_ngram(train_tokenized, n, model_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating the n-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation loop for WittenBell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def lm_perplexity_and_entropy(model, tokenized_test_set):\n",
        "    \"\"\"\n",
        "    Computes Cross-Entropy and Perplexity using model probabilities ONLY.\n",
        "    Fully theory-aligned.\n",
        "    \"\"\"\n",
        "    n = model.order\n",
        "    total_log_prob = 0.0\n",
        "    total_ngrams = 0\n",
        "\n",
        "    for sent in tokenized_test_set:\n",
        "        if not sent:\n",
        "            continue\n",
        "\n",
        "        padded = ['<s>'] * (n - 1) + sent + ['</s>']\n",
        "        for ng in ngrams(padded, n):\n",
        "            context, word = ng[:-1], ng[-1]\n",
        "            log_score = model.logscore(word, context)\n",
        "\n",
        "            if math.isfinite(log_score):\n",
        "                total_log_prob += log_score\n",
        "                total_ngrams += 1\n",
        "\n",
        "    if total_ngrams == 0:\n",
        "        return {\n",
        "            \"CrossEntropy\": float(\"inf\"),\n",
        "            \"Perplexity\": float(\"inf\")\n",
        "        }\n",
        "\n",
        "    cross_entropy = -total_log_prob / total_ngrams\n",
        "    perplexity = 2 ** cross_entropy\n",
        "\n",
        "    return {\n",
        "        \"CrossEntropy\": cross_entropy,\n",
        "        \"Perplexity\": perplexity\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "final_results = {}\n",
        "n_orders = [3, 4, 5]\n",
        "\n",
        "for n in n_orders:\n",
        "    model_path = f\"twi_wittenBell_{n}gram.pkl\"\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            model = pickle.load(f)\n",
        "\n",
        "        metrics = lm_perplexity_and_entropy(\n",
        "            model,\n",
        "            test_tokenized\n",
        "        )\n",
        "\n",
        "        final_results[n] = metrics\n",
        "\n",
        "        print(f\"\\nResults for {n}-gram:\")\n",
        "        print(f\" - Perplexity: {metrics['Perplexity']:.2f}\")\n",
        "        print(f\" - Cross-Entropy: {metrics['CrossEntropy']:.3f}\")\n",
        "        # print(f\" - Coverage: {metrics['Coverage']:.2f}%\")\n",
        "\n",
        "        # if metrics[\"OOV\"] is not None:\n",
        "        #     print(f\" - OOV Rate: {metrics['OOV']:.2f}%\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Warning: Model file {model_path} not found.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation loop with Kneser-Ney"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import pickle\n",
        "import time\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. Optimized Evaluation Functions ---\n",
        "\n",
        "def fast_metrics(model, tokenized_test_set):\n",
        "    \"\"\"\n",
        "    Computes Perplexity, Cross-Entropy, and Coverage.\n",
        "    Uses Counter to group repeated Twi phrases for 10x speedup.\n",
        "    \"\"\"\n",
        "    n = model.order\n",
        "    ngram_counts = Counter()\n",
        "    \n",
        "    # Extract n-grams from test set\n",
        "    for sent in tokenized_test_set:\n",
        "        ngram_counts.update(ngrams(sent, n, pad_left=True, pad_right=True, \n",
        "                                   left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"))\n",
        "\n",
        "    log_prob_sum = 0.0\n",
        "    total_ngrams = 0\n",
        "    known_ngrams = 0\n",
        "\n",
        "    # Batch process unique n-grams\n",
        "    for ng, freq in tqdm(ngram_counts.items(), desc=f\"Evaluating {n}-gram\", leave=False):\n",
        "        word = ng[-1]\n",
        "        context = ng[:-1]\n",
        "        \n",
        "        # Use logscore (base 2) for direct entropy calculation\n",
        "        lp = model.logscore(word, context)\n",
        "        \n",
        "        # Check for coverage (probability > 0)\n",
        "        if lp != float(\"-inf\"):\n",
        "            known_ngrams += freq\n",
        "            log_prob_sum += (lp * freq)\n",
        "        else:\n",
        "            # Apply a penalty for unknown patterns to avoid infinite perplexity\n",
        "            log_prob_sum += (-100 * freq) \n",
        "            \n",
        "        total_ngrams += freq\n",
        "\n",
        "    # Metric Calculations\n",
        "    avg_log_prob = log_prob_sum / total_ngrams\n",
        "    cross_entropy = -avg_log_prob\n",
        "    perplexity = math.pow(2, cross_entropy)\n",
        "    coverage = (known_ngrams / total_ngrams) * 100\n",
        "\n",
        "    return perplexity, cross_entropy, coverage\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. Execution Loop ---\n",
        "\n",
        "final_results = {}\n",
        "n_orders = [3, 4, 5]\n",
        "\n",
        "for n in n_orders:\n",
        "    model_path = f\"twi_kneser_ney_{n}gram.pkl\"\n",
        "    \n",
        "    if os.path.exists(model_path):\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            model = pickle.load(f)\n",
        "        \n",
        "        ppl, entropy, cov = fast_metrics(model, test_tokenized)\n",
        "        \n",
        "        final_results[n] = {\n",
        "            \"Perplexity\": ppl,\n",
        "            \"Cross-Entropy\": entropy,\n",
        "            \"Coverage\": cov\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nResults for {n}-gram:\")\n",
        "        print(f\" - Perplexity: {ppl:.2f}\")\n",
        "        print(f\" - Cross-Entropy: {entropy:.4f} bits\")\n",
        "        print(f\" - Coverage: {cov:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Warning: Model file {model_path} not found.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Specify which model you want to check (e.g., the 3-gram model)\n",
        "model_path = \"twi_kneser_ney_3gram.pkl\"\n",
        "\n",
        "with open(model_path, \"rb\") as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "# This will print the actual number of unique tokens the model learned\n",
        "print(len(loaded_model.vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Sample Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# --- 1. LOAD THE MODEL ---\n",
        "MODEL_PATH= \"twi_wittenBell_{n}gram.pk1\" #BestWittenBell Model\n",
        "# MODEL_PATH = \"twi_kneser_ney_3gram.pkl\" #Best Kneser-Ney Model\n",
        "\n",
        "with open(MODEL_PATH, \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# --- 2. PRE-COMPUTE \"KNOWN FOLLOWERS\" ---\n",
        "# This is the secret to speed. We build a map of tokens that actually \n",
        "# exist for specific contexts so we don't scan the whole vocab.\n",
        "print(\"Optimizing search space...\")\n",
        "context_map = defaultdict(list)\n",
        "for ngram in model.counts[model.order]:\n",
        "    context = ngram[:-1]\n",
        "    word = ngram[-1]\n",
        "    context_map[context].append(word)\n",
        "\n",
        "# --- 3. IMPLEMENT FASTER GENERATOR ---\n",
        "def generate_twi_ultra_fast(model, context_map, max_len=15, temp=0.7, top_k=5):\n",
        "    content = [\"<s>\"]\n",
        "    \n",
        "    for _ in range(max_len):\n",
        "        context = tuple(content[-(model.order-1):])\n",
        "        \n",
        "        # SPEED FIX: Only look at words that actually follow this context\n",
        "        # If the context is unknown, the model would back off anyway.\n",
        "        potential_words = context_map.get(context, list(model.vocab))\n",
        "        \n",
        "        candidates = []\n",
        "        # Even if we check the whole vocab, we limit it to a smaller sample\n",
        "        # if the context_map is empty to maintain speed.\n",
        "        search_list = potential_words if len(potential_words) < 500 else list(model.vocab)[:500]\n",
        "\n",
        "        for token in search_list:\n",
        "            if token in [\"[UNK]\", \"[PAD]\", \"<s>\"]: continue\n",
        "            \n",
        "            # logscore is faster than score in NLTK\n",
        "            ls = model.logscore(token, context)\n",
        "            if ls != float(\"-inf\"):\n",
        "                candidates.append((token, math.pow(2, ls)))\n",
        "        \n",
        "        if not candidates: break\n",
        "            \n",
        "        # Top-K Pruning\n",
        "        candidates = sorted(candidates, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        tokens, scores = zip(*candidates)\n",
        "        \n",
        "        # Temperature Scaling\n",
        "        preds = np.array(scores)\n",
        "        preds = np.exp(np.log(preds + 1e-10) / temp)\n",
        "        preds /= preds.sum() \n",
        "        \n",
        "        next_token = np.random.choice(tokens, p=preds)\n",
        "        content.append(next_token)\n",
        "        if next_token == \"</s>\": break\n",
        "            \n",
        "    return \" \".join([t for t in content if t not in [\"<s>\", \"</s>\"]])\n",
        "\n",
        "# --- 4. EXECUTION ---\n",
        "print(\"\\nGenerating 10 Twi Sentences (High-Speed Mode):\")\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}. {generate_twi_ultra_fast(model, context_map)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myvenv (3.14.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
